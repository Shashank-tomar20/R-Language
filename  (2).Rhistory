summary(mymodel)
plot(mymodel,data=iris,Petal.Width~Petal.Length , slice = list(Sepal.Width = 3 ,Sepal.Length=4))
#Confusion Matrix and Misclassification Error
pred <- predict(mymodel , iris)
tab <- table(Predicted=pred , Actual = iris$Species)
tab
#error
1-sum(diag(tab))/sum(tab)
library(caret)
library(randomForest)
library(e1071)
library(dplyr)
library(xgboost)
install.packages("caret")
install.packages("caret")
library(caret)
# Explore the data
head(iris)
str(iris)
# visulise the data
featurePlot(x=iris[,1:4],y=iris$Species, plot="ellipse")
featurePlot(x=iris[,1:4],y=iris$Species,plot="ellipse",auto.key=list(columns=3))
featurePlot(x=iris[,1:4],y=iris$Species,plot=c("boxplot"),auto.key=list(columns=3),scale=list(y=list(relation="free")),labels=c("Species","length in cm"))
featurePlot(x=iris[,1:4],y=iris$Species,plot="density",auto.key=list(columns=3),labels=c("Species",""),scale=list(x=list(relation="free"),y=list(relation="free")),pch="|")
featurePlot(x=iris[,1:3],y=iris[,4],plot="scatter",type=c("p","smooth"),auto.key=list(columns=3),pch=16,lwd=2)
featurePlot(x=iris[,1:4],y=iris[,4],plot="scatter",type=c("p","smooth"),auto.key=list(columns=3),pch=16,lwd=2)
# See correlation
library(corrplot)
cor_iris=cor(iris[,1:4])
corrplot(cor_iris, type="upper",method="ellipse")
# decide cross validation
trc= trainControl(method="repeatedcv",number=10,repeats=3)
trc
#Fit model
mod_lm = train(Sepal.Length~.,iris,method="lm",trControl=trc)
mod_lm
summary(mod_lm)
mod_lm$resample
mod_lm$results
# diagnostic plots
plot(mod_lm$finalModel)
plot(1:nrow(iris),iris$Sepal.Length)
points(1:nrow(iris),obs, col="red")
# prediction from new data
df_new=data.frame(Sepal.Width=c(4.8,4.6), Petal.Length=c(3.4,6.9),
Petal.Width=c(1.7,2.4), Species=c("setosa","virginica"))
predict(mod_lm,df_new)
# other models using caret : classification models
#support vector machine
mod_svm= train(Species~.,iris,method="svmRadial",trControl=trc)
mod_svm
obs=predict(mod_svm, iris)
confusionMatrix(iris$Species,obs)
plot(mod_svm)
#Gradient Boosting Machine
mod_gbm= train(Species~.,iris,method="gbm",trControl=trc)
mod_gbm
obs=predict(mod_gbm, iris)
confusionMatrix(iris$Species,obs)
plot(mod_gbm)
#Learning Vector Quantization  model
mod_lvq= train(Species~.,iris,method="lvq",trControl=trc)
mod_lvq
obs=predict(mod_lvq, iris)
confusionMatrix(iris$Species,obs)
plot(mod_lvq)
#Random Forest  model
mod_rf= train(Species~.,iris,method="rf",trControl=trc)
mod_rf
obs=predict(mod_rf, iris)
confusionMatrix(iris$Species,obs)
plot(mod_rf)
plot(mod_rf)
View(df_new)
View(mod_gbm)
View(mod_lm)
View(mod_lvq)
View(mod_rf)
View(mod_svm)
View(trc)
datasets::AirPassengers
summary(AirPassengers)
plot(AirPassengers)
abline(reg-lm(AirPassengers-time(AirPassengers)))
plot(aggregate(AirPassengers,FUN=mean))
cycle(AirPassengers)
data("AirPassengers")
data("AirPassengers")
class(AirPassengers)
start(AirPassengers)
end(AirPassengers)
frequency(AirPassengers)
abline(reg=lm(AirPassengers-time(AirPassengers)))
abline(reg=lm(AirPassengers~time(AirPassengers)))
boxplot(AirPassengers-cycle(AirPassengers))
boxplot(AirPassengers~cycle(AirPassengers))
plot(diff(log(AirPassengers)))
library(tseries)
library(tseries)
adf.test(diff(log(AirPassengers)),alternative = c("stationary","explosive"),k=0,)
adf.test(diff(log(AirPassengers)),alternative = c("stationary","explosive"),)
plot(log(AirPassengers))
plot(diff(log(AirPassengers)))
acf(AirPassengers)
acf(diff(log(AirPassengers)))
#AR I MA
#p  d  q
acf(AirPassengers)  #determines the value of q
acf(diff(log(AirPassengers))) #determines the value of p
pacf(diff(log(AirPassengers)))
pacf(diff(log(AirPassengers)))
#lets fit an arima model and predict the future 10 years
#c(p,d,q)
fit <- arima(log(AirPassengers),c(0,1,1),seasonal=list(order=c(0,1,1),period=12))
pred <- preict(fit,n.ahead = 10*12) # years * months
pred <- predict(fit,n.ahead = 10*12) # years * months
pred1 <- 2.718 ^ pred$pred  # e value
ts.plot(AirPassengers,2.718^pred$pred, log ="y",lty =c(1,3))
#testing our model
datawide <- ts(AirPassengers,frequency=12,start=c(1949,12),end=c(1959,12))
fit <- arima(log(datawide),c(0,1,1),seasonal=list(order=c(0,1,1),period=12))
pred1 <- 2.718^pred$pred
data1 <- head(pred1,12)
predicted_1960 <- round(data1,digits=0) #round off
original_1960 <- tail(Airpassengers,12)
original_1960 <- tail(AirPassengers,12)
ts.plot(AirPassengers,2.718^pred$pred ,log="y",lty=c(1,3))
predicted_1960 <- round(data1,digits=0) #round off
#testing our model
datawide <- ts(AirPassengers,frequency=12,start=c(1949,12),end=c(1959,12))
fit <- arima(log(datawide),c(0,1,1),seasonal=list(order=c(0,1,1),period=12))
pred<- prdict(fit,n.ahead = 10*12)
pred<- predict(fit,n.ahead = 10*12)
pred1 <- 2.718^pred$pred
data1 <- head(pred1,12)
predicted_1960 <- round(data1,digits=0) #round off
original_1960 <- tail(AirPassengers,12)
ts.plot(AirPassengers,2.718^pred$pred ,log="y",lty=c(1,3))
View(pred)
View(fit)
View(pred)
original_1960 <- tail(AirPassengers,12)
data1 <- head(pred1,12)
#testing our model
datawide <- ts(AirPassengers,frequency=12,start=c(1949,12),end=c(1959,12))
#testing our model
datawide <- ts(AirPassengers,frequency=12,start=c(1949,12),end=c(1959,12))
datawide
fit <- arima(log(datawide),c(0,1,1),seasonal=list(order=c(0,1,1),period=12))
pred<- predict(fit,n.ahead = 10*12)
pred1 <- 2.718^pred$pred
data1 <- head(pred1,12)
predicted_1960 <- round(data1,digits=0)
predicted_1960
original_1960 <- tail(AirPassengers,12)
original_1960
pred1
fit <- arima(log(datawide),c(0,2,1),seasonal=list(order=c(0,1,1),period=12))
pred<- predict(fit,n.ahead = 10*12)
pred1 <- 2.718^pred$pred
data1 <- head(pred1,12)
-
predicted_1960 <- round(data1,digits=0)
original_1960 <- tail(AirPassengers,12)
ts.plot(AirPassengers,2.718^pred$pred ,log="y",lty=c(1,3))
original_1960
predicted_1960
data <- read.csv("C:/Users/Shashank/OneDrive/Desktop/temp.csv")
data <- read.csv("C:\Users\Shashank\OneDrive\Desktop\temp.csv")
data <- read.csv("C:/Users/Shashank/OneDrive/Desktop/sunspot.csv")
View(data)
class(data)
class(data.frame(data))
head(data)
tail(data)
dim(data)
str(data)
row.names(data)
attach(data)
plot.ts(sunspots)
pred<-diff(log(sunspots))
pred
plot.ts(pred)
adf.test(pred)
adf.test(pred)
acf(data)
acf(lgo(data))
acf(log(data))
acf(log(sunspots))
#time series data
data("AirPassengers")
AP <- AirPassengers
#time series data
data("AirPassengers")
ap <- AirPassengers
str(ap)
head(ap)
ts(ap,frequency =12,start=c(1949,1))
attributes(ap)
plot(ap)
#log transformation for stationary
ap <- log(ap)
plot(ap)
#decomposition of additive time series
decomp <- decompose(ap)
View(decomp)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=2)
plot(decomp$figure,type="o",xlab = 'month',ylab = 'seasonality index',col='red',las=2)
plot(decomp$figure,type="p",xlab = 'month',ylab = 'seasonality index',col='red',las=2)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=2)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=3)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=1)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=1)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=2)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=3)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=4)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=-2)
plot(decomp$figure,type="b",xlab = 'month',ylab = 'seasonality index',col='red',las=2)
plot(decomp)
library(forecast)
model <- auto.arima(ap)
library(seasonal)
library(timeSeries)
library(forecast)
library(forecast)
model <- auto.arima(ap)
library(forecast)
library(forecast , dependencies = TRUE)
install.packages("forecast")
library(forecast )
model <- auto.arima(ap)
model
attributes(model)
#ACF and PACF plots
acf(model$residuals , main = "Correlogram")
pacf(model$residuals,main = "partial correlogram")
#Ljung - box test
#we are interested p value
Box.test(model$residuals , lag = 20 , type = 'Ljung-Box')
#residual plot
hist(model$residuals ,
col = "blue",
xlab = 'error',
main + "Histogram of residuals",
freq = FALSE)
#residual plot
hist(model$residuals ,
col = "blue",
xlab = 'error',
main + "Histogram of residuals",
freq = FALSE)
#residual plot
hist(model$residuals ,
col = "blue",
xlab = 'error',
main = "Histogram of residuals",
freq = FALSE)
lines(density(model$residuals))
library(ggplot2)
#forecast
forecst(model,48)
library(ggplot2)
autoplot(f)
#forecast
f <- forecst(model,48)
library(ggplot2)
autoplot(f)
#forecast
f <- forecst(model,48)
#forecast
f <- forecast(model,48)
library(ggplot2)
autoplot(f)
accuracy(f)
# Data
# http://kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.html
data <- read.table(file.choose(), header = F, sep = "")
plot(data[,60], type = 'l')
j <- c(5, 105, 205, 305, 405, 505)
sample <- t(data[j,])
plot.ts(sample,
main = "Time-series Plot",
col = 'blue',
type = 'b')
s <- sample(1:100, n)
d <- data[i,]
str(d)
pattern <- c(rep('Normal', n),
rep('Cyclic', n),
rep('Increasing trend', n),
rep('Decreasing trend', n),
rep('Upward shift', n),
rep('Downward shift', n))
# Calculate distances
library(dtw)
distance <- dist(d, method = "DTW")
str(newdata)
# Classification with decision tree
library(party)
tree <- ctree(pattern100~., newdata)
# Data
# http://kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.html
data <- read.table(file.choose(), header = F, sep = "")
# Data
# http://kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.html
data <- read.table(file.choose(), header = F, sep = "")
View(data)
str(data)
plot(data[,60], type = 'l')
j <- c(5, 105, 205, 305, 405, 505)
sample <- t(data[j,])
# Data preparation
n <- 10
s <- sample(1:100, n)
s
i <- c(s,100+s, 200+s, 300+s, 400+s, 500+s)
d <- data[i,]
str(d)
pattern <- c(rep('Normal', n),
rep('Cyclic', n),
rep('Increasing trend', n),
rep('Decreasing trend', n),
rep('Upward shift', n),
rep('Downward shift', n))
# Calculate distances
library(dtw)
distance <- dist(d, method = "DTW")
# Hierarchical clustering
hc <- hclust(distance, method = 'average')
plot(hc,
labels = pattern,
cex = 0.7,
hang = -1,
col = 'blue')
rect.hclust(hc, k=4)
# Data preparation
pattern100 <- c(rep('Normal', 100),
rep('Cyclic', 100),
rep('Increasing trend', 100),
rep('Decreasing trend', 100),
rep('Upward shift', 100),
rep('Downward shift', 100))
newdata <- data.frame(data, pattern100)
str(newdata)
# Classification with decision tree
library(party)
tree
tree <- ctree(pattern100~., newdata)
str(newdata)
# Classification with decision tree
library(party)
tree <- ctree(pattern100~., newdata)
# Classification performance
tab <- table(Predicted = predict(tree, newdata), Actual = newdata$pattern100)
tree <- ctree(pattern100~., newdata)
newdata <- data.frame(data, pattern100)
# Data preparation
pattern100 <- c(rep('Normal', 100),
rep('Cyclic', 100),
rep('Increasing trend', 100),
rep('Decreasing trend', 100),
rep('Upward shift', 100),
rep('Downward shift', 100))
newdata
sum(diag(tab))/sum(tab)
# Classification performance
tab <- table(Predicted = predict(tree, newdata), Actual = newdata$pattern100)
# Data preparation
pattern100 <- c(rep('Normal', 100),
rep('Cyclic', 100),
rep('Increasing trend', 100),
rep('Decreasing trend', 100),
rep('Upward shift', 100),
rep('Downward shift', 100))
tree <- ctree(pattern100~., newdata)
library(wikipediatrend)
install.packages("wikipediatrend")
library(wikipediatrend)
data <- wp_trend(page = "Tom_Brady",
from="2013-01-01",
to ="2015-12-31")
library(ggplot2)
#missing data & log transform
data$count[data$count]
library(ggplot2)
qplot(date,count,data=data)
summary(data)
#missing data & log transform
data$count[data$count]
library(wikipediatrend)
data <- wp_trend(page = "Tom_Brady",
from="2013-01-01",
to ="2015-12-31")
library(wikipediatrend)
data <- wp_trend(page = "Tom_Brady",
from="2013-01-01",
to ="2015-12-31")
install.packages("prophet")
install.packages("covid19.analytics")
# Library
library(covid19.analytics)
library(dplyr)
library(prophet)
library(lubridate)
library(ggplot2)
install.packages("covid19.analytics")
install.packages(c("dplyr", "prophet", "lubridate", "ggplot2"))
# Library
library(covid19.analytics)
library(prophet)
library(lubridate)
library(ggplot2)
# Data
tsc <- covid19.data(case = 'ts-confirmed')
colnames(tsc) <- c('Date', 'Confirmed')
# Data
tsc <- covid19.data(case = 'ts-confirmed')
# Library
library(covid19.analytics)
install.packages("covid19.analytics")
# Library
library(covid19.analytics)
install.packages("cellranger")
# Library
library(covid19.analytics)
library(dplyr)
library(prophet)
library(lubridate)
library(ggplot2)
# Data
tsc <- covid19.data(case = 'ts-confirmed')
tsc <- tsc %>% filter(Country.Region == 'US')
tsc <- data.frame(t(tsc))
tsc <- cbind(rownames(tsc), data.frame(tsc, row.names = NULL))
tsc
colnames(tsc) <- c('Date', 'Confirmed')
tsc$Date <- ymd(tsc$Date)
tsc$Confirmed <- as.numeric(tsc$Confirmed)
# Plot
qplot(Date, Confirmed, data = tsc)
ds <- tsc$Date
y <- tsc$Confirmed
df <- data.frame(ds, y)
# Forecasting
m <- prophet(df)
library(forecast)
library(ggplot2)
source("D:/r Language/ytTS.R")
library(caret)
library(ggplot2)
library(caTools)
library(dplyr)
library(lattice)
mydata <- read.csv("D:/r Language/RELIANCE.csv")
View(mydata)
summary(mydata)
newdata <- select(mydata,-2:-3,-13:-15)
View(newdata)
#splitting mydata
split1 <- sample.split(newdata$Open,SplitRatio = 0.7)
split1
train_data <- subset(newdata,split="TRUE")
test_data <- subset(newdata,split="FALSE")
model <- lm(Turnover~.,data = train_data)
summary(model)
predic <- predict (model,test_data)
predic
#to compare predicted values and actual values we can use plots
plot(test_data$Turnover,type="l",lty =1.8 , col = "green")
lines(predic ,type="l",col="blue")
#to compare predicted values and actual values we can use plots
plot(test_data$Turnover,type="l",lty =1.8 , col = "green")
lines(predic ,type="l",col="blue")
RMSE(predic,test_data$Turnover)
library(lattice)
library(dplyr)
library(caTools)
library(ggplot2)
library(caret)
mydata <- read.csv("D:/r Language/RELIANCE.csv")
library(lattice)
library(dplyr)
library(caTools)
library(ggplot2)
library(caret)
mydata <- read.csv("D:/r Language/RELIANCE.csv")
View(mydata)
summary(mydata)
newdata <- select(mydata,-2:-3,-13:-15)
View(newdata)
#splitting mydata
split1 <- sample.split(newdata$Open,SplitRatio = 0.7)
split1
train_data <- subset(newdata,split="TRUE")
test_data <- subset(newdata,split="FALSE")
model <- lm(Turnover~Volume+Open+Close,data = train_data)
summary(model)
predic <- predict (model,test_data)
predic
#to compare predicted values and actual values we can use plots
plot(test_data$Turnover,type="l",lty =1.8 , col = "green")
lines(predic ,type="l",col="blue")
RMSE(predic,test_data$Turnover)
model <- lm(Turnover~Volume+Open+Close+VWAP,data = train_data)
summary(model)
predic <- predict (model,test_data)
predic
#to compare predicted values and actual values we can use plots
plot(test_data$Turnover,type="l",lty =1.8 , col = "green")
lines(predic ,type="l",col="blue")
RMSE(predic,test_data$Turnover)
plot(model)
